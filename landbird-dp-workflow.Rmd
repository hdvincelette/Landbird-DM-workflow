---
title: "Landbird Data Processing Workflow"
name: Hannah
date: "`r format(Sys.time(), '%B %Y')`"
output:
  html_notebook:
    toc: yes
    number_sections: yes
  html_document:
    toc: yes
    df_print: paged
address: Alaska Regional Office 1011 E Tudor Rd, Anchorage, AK
email: hannah_vincelette@fws.gov
github: "https://github.com/hdvincelette/landbird-dp-workflow"
surname: Vincelette
position: Wildlife Biologist
editor_options:
  chunk_output_type: console
---

# Overview

The purpose of this document is to provide a reproducible workflow for processing data collected annually by the Landbird section of USFWS Alaska Migratory Bird Management. Data processing is just one component of data management. For complete guidance, view the [Alaska Region Interim Data Management User Guide.](https://ak-region-dst.gitbook.io/alaska-region-interim-data-management-user-guide/)

#### What is an R Notebook?

["An R Notebook is an R Markdown document with chunks that can be executed independently and interactively, with output visible immediately beneath the input." - R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/notebook.html)

#### How to use this guide

This document is meant to be interactive, with text prompts and "code chunks" which may require input before being executed. The following workflow was created for tabular data collected annually for projects archived on the Regional Data Repository. While effort is made to generalize code to all actively collected data, further edits may be required to process data in an standardized format. In this case, it is recommended users duplicate and rename the "landbird-dp-workflow.Rmd" before making changes.

# Workspace setup

### Install FWSAkRDRtools R package

Run the following installation code.

```{r include=FALSE, results=TRUE}

# Install missing packages
if (!require("pacman"))
  install.packages("pacman")
pacman::p_load("devtools", "move", "auk", "dplyr")

# (optional) Set GitHub download file method
options(download.file.method = "wininet")

# Install FWSAkRDRtools from GitHub
devtools::install_github("hdvincelette/FWSAkRDRtools")

# Install mdJSONdictio from GitHub
devtools::install_github("hdvincelette/mdJSONdictio")

```

If the installations fail, refer to [Install mdJSONdictio R package](https://hdvincelette.github.io/mdJSONdictio/articles/03_Setup_mdJSONdictio.html#install-mdjsondictio-r-package) for troubleshooting tips.

# Data preprocessing

### Download authoritative copies of project files

Open the [Migratory Bird Management Regional Data Repository](file://ifw7ro-file.fws.doi.net/datamgt/mbm/) and navigate to the project folder. If you are working from a remote location, you must be connected to the VPN (Virtual Private Network) to access this folder.

Paste the names of the project folder, data dictionary(ies), and data entry template(s).

```{r include=FALSE}

project.folder <- "mbmlb_003_Red_Knot_Breeding"
dictionary.list <- c("dictionary")
template.list <- c("template")
local.path <- "C:/Users/hvincelette/OneDrive - DOI/Documents/GitHub/landbird-dp-workflow/downloads"

```

Now run the following code chunk.

```{r include=FALSE, results=FALSE}

FWSAkRDRtools::download.files(
  pattern = c(dictionary.list, template.list),
  project = project.folder,
  path = local.path,
  main = TRUE,
  incoming = TRUE,
  recursive = TRUE
)

```

View the downloaded data dictionary(ies) and data entry template(s) in the "downloads" sub-folder of this R project. You can navigate here using the "Files" tab in the lower left pane. This should default to the project directory. If not, click "Session" in the toolbar, hover over "Set Working Directory," and click "To Project Directory." Click the "downloads" sub-folder, navigate to the blue cog wheel ("More file commands"), and select "Show Folder in New Window." A File Explorer window should open up to the "downloads" sub-folder. Verify all your data dictionaries and data entry templates are contained here.

*Optional: You can skip the above steps and download everything manually from the [Migratory Bird Management Regional Data Repository](file://ifw7ro-file.fws.doi.net/datamgt/mbm/) project folder. Data dictionaries and data entry templates should be available in the "metadata" sub-folder. Save a copy of data dictionary(ies) and data entry template(s) in an accessible location (i.e. the "downloads" sub-folder of this R project)*

### Enter scanned or hard copy data

Copy and save each data entry template in a preferred location for data entry (this can be in the "data-raw" sub-folder of this R project). Use a unique and descriptive name - refer to [Best Practices in Naming Conventions](https://ak-region-dst.gitbook.io/alaska-region-interim-data-management-user-guide/alaska-data-management-101/file-organization-and-best-practices/best-practices-in-naming-conventions). It's okay to save this file in an Excel file format to preserve validation rules.

Enter scanned or hard copy data according to the respective data dictionary(ies). Review entered data to check for errors. The Excel Filter function (Home\>Editing\>Filter) is especially useful for viewing unique values and value ranges.

Save the entered data file(s) in the "data-raw" sub-folder of this R project. Convert the file(s) to comma-separated values (CSV) format, if necessary. Make sure the correct sheet is selected when converting if more than one sheet is used (i.e., validation rules).

# Data quality checks

Before beginning, make sure all entered data and data dictionary(ies) are saved in the sub-folders of this R project ("data-raw" and "downloads", respectively).

### Summarize: capture, resight, nest, survey

SpeciesCode, Date, Location, Attachments (bands, tags), Morphometrics

```{r}
# Provide directory paths for the data file(s) and dictionary
raw.data.path<- "data-raw/"
dictionary.path<- "downloads/"

```

```{r}
# Import raw data file(s)
raw.data.files <-
  list.files(
    path = raw.data.path,
    pattern = NULL,
    full.names = FALSE,
    recursive = FALSE
  )

raw.data.choice <- utils::select.list(
  c(raw.data.files),
  multiple = TRUE,
  graphics = TRUE,
  title = cat(paste0("\nSelect one or more data files to import."))
)

raw.data.ext <- tools::file_ext(raw.data.choice)
import.data.name <-
  stringr::str_replace(raw.data.choice, paste0(".", raw.data.ext), "")


while (TRUE %in% duplicated(import.data.name)) {
  message(cat(
    "\n\n\nError: The following files have the same name.",
    paste0("\n  ", raw.data.choice[dup.test])
  ))
  
  raw.data.choice <- utils::select.list(
    c(raw.data.files),
    multiple = TRUE,
    graphics = TRUE,
    title = cat(paste0(
      "\n...Reselect one or more data files to import."
    ))
  )
  
  raw.data.ext <- tools::file_ext(raw.data.choice)
  import.data.name <-
    stringr::str_replace(raw.data.choice, paste0(".", raw.data.ext), "")
  
}

raw.data.list <- list()

for (a in 1:length(raw.data.ext)) {
  if (raw.data.ext[a] %in% c("xlsx", "xls")) {
    import.file <-
      readxl::read_excel(paste0(raw.data.path, raw.data.choice[a]))
    raw.data.list[[a]] <- import.file
    
  } else if (raw.data.ext[a] %in% c("csv")) {
    import.file <-
      utils::read.csv(paste0(raw.data.path, raw.data.choice[a]))
    raw.data.list[[a]] <- import.file
    
  }
}

names(raw.data.list) <- import.data.name


# Import data dictionary
dictionary.files <-
  list.files(
    path = dictionary.path,
    pattern = NULL,
    full.names = FALSE,
    recursive = FALSE
  )

dictionary.choice <- utils::select.list(
  c(dictionary.files),
  multiple = FALSE,
  graphics = TRUE,
  title = cat(paste0("\nSelect a dictionary to import."))
)


dictionary.ext <- tools::file_ext(dictionary.choice)

if (dictionary.ext %in% c("xlsx", "xls")) {
  ref.dictionary <-
    readxl::read_excel(paste0(dictionary.path, dictionary.choice))
  
} else if (dictionary.ext %in% c("csv")) {
  ref.dictionary <-
    utils::read.csv(paste0(dictionary.path, dictionary.choice))
  
} else if (dictionary.ext %in% c("json")) {
  ref.dictionary <-
    rjson::fromJSON(file = paste0(getwd(), "/", dictionary.path, dictionary.choice))
  
}



```

### Validate: data file(s) v. dictionary

```{r}
# Validate data file(s) against the dictionary
warning.list <- list()

for (a in 1:length(raw.data.list)) {
  datatype.rules <-
    get0("datatype.rules", envir = asNamespace("mdJSONdictio"))
  
  assign("input.dict", ref.dictionary)
  
  assign("input.data", raw.data.list[[a]])
  
  if (dictionary.ext %in% c("json")) {
    warning.list[[a]] <-
      validate.mdJSON(
        x = input.dict,
        y = input.data
      )
  } else {
    warning.list[[a]] <-
      mdJSONdictio::validate.table(x = input.dict,
                                   y = input.data)
  }
}

names(warning.list) <- import.data.name
                                                        

```

### Reconcile: correct data files

```{r}
# display unique values..readlines / replace

```

### Reconcile: correct data dictionary

```{r}


req.attribute.fields <-
      c("codeName", "allowNull", "dataType", "definition", "domainId")


for (a in 1:length(warning.list)) {
  warn.table <- warning.list[[a]]
  warn.table$Category[warn.table$Category == "CodeName"] <- "codeName"
  
  codeName.warn<- warn.table$Variable[warn.table$Category=="codeName"]
  
  codeName.choice <- utils::select.list(
    c(codeName.warn, "None"),
    multiple = TRUE,
    graphics = TRUE,
    title = cat(
      paste0(
        "\nSome variables in the data file '",
        import.data.name[a],
        "' were not found in the data dictionary. Select which, if any, to add to the data dictionary"
      )
    )
  )
  
  for(b in 1:length(codeName.choice)){
  new.dictionary <-
    modify.mdJSON(x = ref.dictionary,
                  how = "add_attribute",
                  codeName = codeName.choice[a])
  
  }

  
  check.dictionary <-
    rjson::fromJSON(new.dictionary[["data"]][[1]][["attributes"]][["json"]])
  
  
  new.json = rjson::toJSON(x = new.dictionary)
  write(x = new.json, file = "test.dictionary.json")

}

  
```

### Quality control checks: visualization

```{r}
# continuous: box plots
# discrete: range of values (interactive table)
# plot lat/long
# uniqe value check - band number, flag code, colorband combo
# BBL status

```

# Merge data file with authoritative copy

```{r}
#
```

# Data quality checks (cont.)

### Validation: data file v. dictionary

```{r}
#
```

# Data postprocessing

uuids, versioning

```{r}
#
```

# Archive in the Regional Data Repository

```{r}
#
```

# Capture data transformations

### Biological samples inventory

```{r}
#
```

### Banders portal

```{r}
#
```

# Survey data transformations

### ebird

```{r}
library("auk")

# ebird
# usfwslandbirds
# fwslandbirds
# API key: e2m7tt6sfo4n


```

# Tracking data

### Movebank import

Verify the stored login credentials are correct, and add/remove credentials as needed.

```{r warning=FALSE, include=FALSE, results=FALSE}

# Check existing login credentials
keyring::key_list()

# Add login credentials
move2::movebank_store_credentials(username = "mbmlandbirds",
                                  password = "AKmove2018!",
                                  key_name = "myOtherAccount")

# Remove login credentials
move2::movebank_remove_credentials(key_name = getOption("move2_movebank_key_name"))


```

*Optional: If you want to import Movebank data from a secondary account, paste the associated service name listed in the key list (e.g., "myOtherAccount") and run the following code. Note the account with the service name "movebank" is the default, so you only need to run this code if you want to access a different account. Restart R or execute* `options("move2_movebank_key_name" = "movebank")` *to* *return to the default account. See* `vignette("movebank", package="move2")` *for more details.*

```{r}

keyring::key_list()

options("move2_movebank_key_name" = "myOtherAccount")

```

Run the following code to import location data from one or more studies into R. Single study imports have the option of selecting a time range of location data, whereas multiple study imports include all location data. By default, the location and reference data are both imported

```{r warning=FALSE, include=FALSE, results=FALSE}
          
# Import Movebank location and/or reference data
import <- get.move.data(
  location = TRUE,
  reference = TRUE,
  remove_movebank_outliers <- FALSE,
  omit_derived_data <- FALSE
)

save(import, import.2, file = "man/import.RData")

# Unlist dataframes (if necessary)
if(any(sapply(import, function(x)
  any(class(x) == "list")))==TRUE) {
  import.dfs <- rrapply::rrapply(
    import,
    classes = "data.frame",
    how = "flatten",
    options = list(namesep = "-",
                   simplify = FALSE)
  )
} else {
  import.dfs<- import
}

# Update dataField names
import.dfs <-
  lapply(import.dfs, function(x)
    setNames(x,
             sub(
               pattern = "_", replacement = "-", names(x)
             )))

import.dfs <-
  lapply(import.dfs, function(x)
    setNames(x,
             sub(
               pattern = "argos-", replacement = "argos:", names(x)
             )))


#!# only grabs the first items colnames
df<- tibble::as_tibble.list(import.dfs[[1]])
loc.cols<- colnames(dplyr::select(df, matches("location")))


names(import.dfs[[1]][["argos:location_1"]][[1]][1])
import.dfs[[1]][["argos:location_1"]][[1]][[1]]


renquote <- function(l) if (is.list(l)) lapply(l, renquote) else enquote(l)
test<- lapply(unlist(renquote(import.dfs[1])), eval)
test2<- lapply(unlist(renquote(test)), eval)



for(a in 1:length(loc.cols)) {
  for (b in 1:length(df)) {
    if (sf::st_is_empty(df[b, loc.cols[a]]) == FALSE) {
      unnested.loc<- as.data.frame(sf::as_Spatial(df[b, loc.cols[a]]))
      colname.loc<- colnames(unnested.loc)
      unnested.long<- unnested.loc[1]
      unnested.lat<- unnested.loc[2]
      if(!colnames(unnested.long) %in% colnames(df)){
        df %>%
          tibble::add_column(colname.loc[1] = NA,
                             .before = paste0(loc.cols[a]))
      }
      if(!colnames(unnested.lat) %in% colnames(df)){
        
      }
      df$colnames(unnested.long)[b] <- unnested.long
      df$colnames(unnested.lat)[b] <- unnested.lat
    }
  }
}




# Save dataframes as separate objects 
for (i in 1:length(import.2)){
   assign(names(import.2[i]),as.data.frame(import.2[[i]]))
}
  

# Get updated movebank vocabulary
move.vocab <-
  move2::movebank_get_vocabulary(return_type = "list", omit_deprecated = FALSE)

move.vocab <- rrapply::rrapply(
  move.vocab,
  how = "bind",
  options = list(
    simplify = FALSE,
    coldepth = 3,
    namecols = TRUE
  )
) %>%
  tidyr::pivot_wider(
    names_from = "L2",
    values_from = "1",
    values_fill = NA,
    values_fn = unique
  ) %>%
  tidyr::unnest(colnames(.)) %>%
  dplyr::rename("dataField" = "L1") %>%
  dplyr::mutate(dataField = stringr::str_replace_all(dataField, c("argos " = "argos:", " " = "-")),
                altLabel = stringr::str_replace_all(altLabel, c("argos " = "argos:", " " = "-")))
  

colnames(test)[!colnames(test) %in% move.vocab$dataField]



```

### Transformations
